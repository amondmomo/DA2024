{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fda856d-73d3-4cf1-ad5a-9c9403f9f5a8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ZERO-Shot Image Classification\"\n",
    "author: \"이정민\"\n",
    "date: \"02/09/2024\"\n",
    "categories:\n",
    "  - Deep Learning\n",
    "  - zero-shot image classification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc44e5-a3cf-4be7-9dca-dd1bdde61c63",
   "metadata": {},
   "source": [
    "# 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96c1951-5bae-4cef-90e4-e91899553130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import os, json, open_clip, torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7dedc-fedc-4d94-8e43-6782407186bf",
   "metadata": {},
   "source": [
    "# 2. open clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03051a88-11d1-4756-ba66-cca9995a8743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## open_clip 모델 확인, [('model_name','pretrained')]\n",
    "open_clip.list_pretrained()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72656389-9b40-4f70-967d-dc78f3db0237",
   "metadata": {},
   "source": [
    "# image classificaion 기본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e43ce43b-0480-4d08-bc0b-331abb09c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 254/254 [01:10<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import os, json, open_clip, torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 0. Settings\n",
    "device = 'cuda:0'\n",
    "model_name = 'ViT-B-16'\n",
    "pretrained = 'openai'\n",
    "root = './'\n",
    "dataset_name = 'Scene'\n",
    "\n",
    "# 1. Load CLIP model.\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name='ViT-bigG-14-CLIPA',\n",
    "    pretrained='datacomp1b',\n",
    "    force_patch_dropout=0.05\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-bigG-14-CLIPA')\n",
    "\n",
    "# 2. Load test dataset.\n",
    "ds = ImageFolder(os.path.join(root, dataset_name), transform=preprocess)\n",
    "ds.samples = natsorted(ds.samples)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32, num_workers=2)\n",
    "\n",
    "# 3. Load class name list.\n",
    "with open(os.path.join(root, 'classes.json'), 'r') as j:\n",
    "     class_names = json.loads(j.read())\n",
    "\n",
    "# 4. Perform zero-shot classification.\n",
    "zero_shot_top1 = 0\n",
    "submission = dict({'id_idx':list(range(8100)), 'label':[]})\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text = tokenizer([f\"{class_name}\" for class_name in class_names])\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    model = model.to(device)\n",
    "    for x, y in tqdm(dl):\n",
    "        x = x.cuda(device)\n",
    "        image_features = model.encode_image(x).to('cpu').float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        zero_shot_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        zero_shot_pred = zero_shot_probs.max(dim=-1)[1].tolist()\n",
    "        submission['label'] += zero_shot_pred\n",
    "\n",
    "# 5. Save prediction as submission.scv file.\n",
    "pd.DataFrame(submission).to_csv(os.path.join(root, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d336065-ce6d-4f7a-bd5b-13f5d87bb152",
   "metadata": {},
   "source": [
    "- SCORE : 0.88790"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc14101-d9fc-484d-86f7-2968b1cac382",
   "metadata": {},
   "source": [
    "## 이미지 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d495ad-5fe5-4b74-b464-495011784271",
   "metadata": {},
   "source": [
    "### CROP_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b34ce-f604-49a5-93e6-f6fec6b48bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((int(224 * 0.95), int(224 * 0.95))),\n",
    "    preprocess\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "ds = ImageFolder(os.path.join(root, dataset_name), transform=crop_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3a5a4-a0fb-48e0-886a-5afaca152f74",
   "metadata": {},
   "source": [
    "## 점수 제일 많이 나왔던 코드(검증 파일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3017e44-0b64-4fd8-87bc-5540fde466c0",
   "metadata": {},
   "source": [
    "### classes\n",
    "classes :\n",
    "\n",
    "[\"Buildings\",\"Forests\",\"Glacier\",\"Mountains\",\"Sea\",\"Street\"]\n",
    "\n",
    "### classes2 :\n",
    "\n",
    "[\"Building\",\"Forest\",\"Glacier\",\"Mountains\",\"Sea\",\"Street\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf30f5-4441-4506-9509-b9f579a22d00",
   "metadata": {},
   "source": [
    "### - model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae42f6a-4b37-4e1d-9ffa-71e646419264",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScene\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mViT-bigG-14-CLIPA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatacomp1b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-bigG-14-CLIPA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/factory.py:399\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[1;32m    376\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    377\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    395\u001b[0m ):\n\u001b[1;32m    396\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    397\u001b[0m         {}, mean\u001b[38;5;241m=\u001b[39mimage_mean, std\u001b[38;5;241m=\u001b[39mimage_std, interpolation\u001b[38;5;241m=\u001b[39mimage_interpolation, resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode)\n\u001b[0;32m--> 399\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[1;32m    419\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[1;32m    420\u001b[0m         pp_cfg,\n\u001b[1;32m    421\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[1;32m    423\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/factory.py:267\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         model \u001b[38;5;241m=\u001b[39m CustomTextCLIP(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_cfg, cast_dtype\u001b[38;5;241m=\u001b[39mcast_dtype)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    270\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m precision \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbfloat16\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/model.py:237\u001b[0m, in \u001b[0;36mCLIP.__init__\u001b[0;34m(self, embed_dim, vision_cfg, text_cfg, quick_gelu, init_logit_scale, init_logit_bias, cast_dtype, output_dict)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dict \u001b[38;5;241m=\u001b[39m output_dict\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual \u001b[38;5;241m=\u001b[39m \u001b[43m_build_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquick_gelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m text \u001b[38;5;241m=\u001b[39m _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtransformer\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/model.py:148\u001b[0m, in \u001b[0;36m_build_vision_tower\u001b[0;34m(embed_dim, vision_cfg, quick_gelu, cast_dtype)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vision_cfg\u001b[38;5;241m.\u001b[39mact_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         act_layer \u001b[38;5;241m=\u001b[39m partial(act_layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvision_cfg\u001b[38;5;241m.\u001b[39mact_kwargs)\n\u001b[0;32m--> 148\u001b[0m     visual \u001b[38;5;241m=\u001b[39m \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mls_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mls_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattentional_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentional_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_pooler_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_pooler_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_pooler_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_pooler_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_embed_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_ln_pre\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_ln_pre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_ln_after_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_ln_after_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visual\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/transformer.py:491\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[0;34m(self, image_size, patch_size, width, layers, heads, mlp_ratio, ls_init_value, attentional_pool, attn_pooler_queries, attn_pooler_heads, output_dim, patch_dropout, no_ln_pre, pos_embed_type, pool_type, final_ln_after_pool, act_layer, norm_layer, output_tokens)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_dropout \u001b[38;5;241m=\u001b[39m PatchDropout(patch_dropout) \u001b[38;5;28;01mif\u001b[39;00m patch_dropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity() \u001b[38;5;28;01mif\u001b[39;00m no_ln_pre \u001b[38;5;28;01melse\u001b[39;00m norm_layer(width)\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mls_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mls_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attentional_pool:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attentional_pool, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/transformer.py:337\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, width, layers, heads, mlp_ratio, ls_init_value, act_layer, norm_layer, batch_first)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    338\u001b[0m     ResidualAttentionBlock(\n\u001b[1;32m    339\u001b[0m         width,\n\u001b[1;32m    340\u001b[0m         heads,\n\u001b[1;32m    341\u001b[0m         mlp_ratio,\n\u001b[1;32m    342\u001b[0m         ls_init_value\u001b[38;5;241m=\u001b[39mls_init_value,\n\u001b[1;32m    343\u001b[0m         act_layer\u001b[38;5;241m=\u001b[39mact_layer,\n\u001b[1;32m    344\u001b[0m         norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[1;32m    345\u001b[0m         batch_first\u001b[38;5;241m=\u001b[39mbatch_first,\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layers)\n\u001b[1;32m    348\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/transformer.py:338\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;241m=\u001b[39m batch_first\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 338\u001b[0m     \u001b[43mResidualAttentionBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mls_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mls_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layers)\n\u001b[1;32m    348\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/open_clip/transformer.py:233\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, d_model, n_head, mlp_ratio, ls_init_value, act_layer, norm_layer, is_cross_attention, batch_first)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2 \u001b[38;5;241m=\u001b[39m norm_layer(d_model)\n\u001b[1;32m    231\u001b[0m mlp_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(d_model \u001b[38;5;241m*\u001b[39m mlp_ratio)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(OrderedDict([\n\u001b[0;32m--> 233\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_fc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_width\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    234\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, act_layer()),\n\u001b[1;32m    235\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mLinear(mlp_width, d_model))\n\u001b[1;32m    236\u001b[0m ]))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls_2 \u001b[38;5;241m=\u001b[39m LayerScale(d_model, ls_init_value) \u001b[38;5;28;01mif\u001b[39;00m ls_init_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/torch/nn/modules/linear.py:109\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl2024/lib/python3.10/site-packages/torch/nn/init.py:459\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    457\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "root = './'\n",
    "dataset_name = 'Scene'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name='ViT-bigG-14-CLIPA',\n",
    "    pretrained='datacomp1b'\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-bigG-14-CLIPA')\n",
    "model = model.to(device)\n",
    "ds = ImageFolder(os.path.join(root, dataset_name), transform=preprocess)\n",
    "ds.samples = natsorted(ds.samples)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32, num_workers=2)\n",
    "\n",
    "with open(os.path.join(root, 'classes.json'), 'r') as j:\n",
    "    class_names = json.loads(j.read())\n",
    "\n",
    "submission = {'id_idx': [], 'label': [], 'probabilities': []}\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text = tokenizer([f\"{class_name}\" for class_name in class_names])\n",
    "    text = text.to(device)\n",
    "    \n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for i, (x, _) in enumerate(dl):\n",
    "        x = x.to(device)  # Move image tensors to GPU\n",
    "        image_features = model.encode_image(x)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        zero_shot_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        for idx, probs in enumerate(zero_shot_probs):\n",
    "            # Get the top prediction and its probability\n",
    "            top_pred = probs.argmax().item()\n",
    "            top_prob = probs.max().item()\n",
    "            \n",
    "            # Collect results for each image\n",
    "            submission['id_idx'].append(i * dl.batch_size + idx)\n",
    "            submission['label'].append(top_pred)\n",
    "            submission['probabilities'].append(probs.tolist())\n",
    "\n",
    "# Save predictions to submission.csv \n",
    "pd.DataFrame(submission).to_csv(os.path.join(root, 'basicBigG14.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060e04a-a6c1-48c8-8f3b-b12b626f5828",
   "metadata": {},
   "source": [
    "### - model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e661c-021e-4fef-b5c0-6577c853c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name='ViT-L-14-CLIPA',\n",
    "    pretrained='datacomp1b',\n",
    "    force_patch_dropout= 0.05\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14-CLIPA')\n",
    "model = model.to(device)\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((int(224 * 0.95), int(224 * 0.95))),\n",
    "    preprocess\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "ds = ImageFolder(os.path.join(root, dataset_name), transform=crop_transform)\n",
    "ds.samples = natsorted(ds.samples)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=64, num_workers=2)\n",
    "\n",
    "# Load class names\n",
    "with open(os.path.join(root, 'classes2.json'), 'r') as j:\n",
    "    class_names = json.loads(j.read())\n",
    "\n",
    "# Perform zero-shot classification\n",
    "submission = {'id_idx': [], 'label': [], 'probabilities': []}\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text = tokenizer([f\"{class_name}\" for class_name in class_names])\n",
    "    text = text.to(device)\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    for i, (x, _) in enumerate(dl):\n",
    "        x = x.to(device) \n",
    "        image_features = model.encode_image(x)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        zero_shot_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        for idx, probs in enumerate(zero_shot_probs):\n",
    "            # Get the top prediction and its probability\n",
    "            top_pred = probs.argmax().item()\n",
    "            top_prob = probs.max().item()\n",
    "            # Collect results for each image\n",
    "            submission['id_idx'].append(i * dl.batch_size + idx)\n",
    "            submission['label'].append(top_pred)\n",
    "            submission['probabilities'].append(probs.tolist())\n",
    "\n",
    "# Save predictions to submission.csv\n",
    "pd.DataFrame(submission).to_csv(os.path.join(root, 'z_L14_cl2_do_64_crop.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7e25d-1988-4067-9288-0ca62341900d",
   "metadata": {},
   "source": [
    "\n",
    "### - model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b09fb-f364-4bdb-acda-52a400d71a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name='EVA02-L-14',\n",
    "    pretrained='merged2b_s4b_b131k',\n",
    "    force_patch_dropout= 0.05\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('EVA02-L-14')\n",
    "model = model.to(device)\n",
    "crop_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((int(224 * 0.95), int(224 * 0.95))),\n",
    "    preprocess\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "ds = ImageFolder(os.path.join(root, dataset_name), transform=crop_transform)\n",
    "ds.samples = natsorted(ds.samples)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=64, num_workers=2)\n",
    "\n",
    "# Load class names\n",
    "with open(os.path.join(root, 'classes2.json'), 'r') as j:\n",
    "    class_names = json.loads(j.read())\n",
    "\n",
    "# Perform zero-shot classification\n",
    "submission = {'id_idx': [], 'label': [], 'probabilities': []}\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text = tokenizer([f\"{class_name}\" for class_name in class_names])\n",
    "    text = text.to(device)\n",
    "    text_features = model.encode_text(text)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    for i, (x, _) in enumerate(dl):\n",
    "        x = x.to(device) \n",
    "        image_features = model.encode_image(x)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        zero_shot_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        for idx, probs in enumerate(zero_shot_probs):\n",
    "            # Get the top prediction and its probability\n",
    "            top_pred = probs.argmax().item()\n",
    "            top_prob = probs.max().item()\n",
    "            # Collect results for each image\n",
    "            submission['id_idx'].append(i * dl.batch_size + idx)\n",
    "            submission['label'].append(top_pred)\n",
    "            submission['probabilities'].append(probs.tolist())\n",
    "\n",
    "# Save predictions to submission.csv\n",
    "pd.DataFrame(submission).to_csv(os.path.join(root, 'z_EVA02_cl2_do_64_crop.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a790d3-dc5b-49b9-aa86-48f3f09f381f",
   "metadata": {},
   "source": [
    "## 소프트 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e09894-7e0f-4ac5-981b-63f92415254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "df1 = pd.read_csv('/root/ajou/basicBigG14.csv')\n",
    "df2 = pd.read_csv('/root/ajou/z_L14_cl2_do_64_crop.csv')\n",
    "df3 = pd.read_csv('/root/ajou/z_EVA02_cl2_do_64_crop.csv')\n",
    "\n",
    "\n",
    "\n",
    "data = df1.assign(probabilities2 = df2.probabilities).assign(probabilities3 = df3.probabilities)\n",
    "\n",
    "\n",
    "data['probabilities'] = data['probabilities'].apply(ast.literal_eval)\n",
    "data['probabilities2'] = data['probabilities2'].apply(ast.literal_eval)\n",
    "data['probabilities3'] = data['probabilities3'].apply(ast.literal_eval)\n",
    "\n",
    "# 확률 평균 계산\n",
    "def soft_voting(row):\n",
    "    probs = np.array(row['probabilities'])\n",
    "    probs2 = np.array(row['probabilities2'])\n",
    "    probs3 = np.array(row['probabilities3'])\n",
    "    avg_probs = (probs + probs2 + probs3 ) / 3\n",
    "    return avg_probs\n",
    "\n",
    "data['soft_voting'] = data.apply(soft_voting, axis=1)\n",
    "\n",
    "# 최종 예측 클래스 계산\n",
    "def final_prediction(row):\n",
    "    return np.argmax(row['soft_voting'])\n",
    "\n",
    "data['final_prediction'] = data.apply(final_prediction, axis=1)\n",
    "data1 = data.drop(['label','probabilities','probabilities2','probabilities3','soft_voting'],axis=1).rename({'final_prediction':'label'},axis=1)\n",
    "\n",
    "data1.to_csv(\"test5.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
